
services:
  # llm-model:
  #   image: ollama/ollama:latest
  #   command: serve
  #   container_name: ollama_llm
  #   environment:
  #     - OLLAMA_MODELS=/root/.ollama/models
  #   ports:
  #     - "11434:11434"
  #   volumes:
  #     - ollama_data:/root/.ollama

  ingester:
    build: . 
    container_name: ingester
    command: ["python", "ingest.py"]
    environment:
      - COLLECTION_NAME=${COLLECTION_NAME}
      - DATA_PATH=${DATA_PATH}
      - VECTOR_STORE_PATH=${VECTOR_STORE_PATH}
      - API_KEY=${API_KEY}
    volumes:
      - ./chroma_db:/app/chroma_db  # Added: shared volume with rag-api
      - ./data:/app/data 
    

  rag-api:
    build: . # Build from the current directory (using our Dockerfile)
    container_name: rag_api
    command: ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
    ports:
      - "8000:8000"
    volumes:
      - ./chroma_db:/app/chroma_db
      - .:/app
    depends_on:
      - ingester
    environment:
      - LLM_MODEL=${LLM_MODEL}
      - API_KEY=${API_KEY}
      - COLLECTION_NAME=${COLLECTION_NAME}
      - VECTOR_STORE_PATH=${VECTOR_STORE_PATH}
   
# volumes:
#   ollama_data: 